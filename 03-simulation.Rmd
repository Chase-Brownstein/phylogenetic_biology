# Simulation

Build the machinery to simulate the evolution of traits on trees. Focus for now on DNA evolution.


## Models

Generative models

The intent is a simplified representation of the process under consideration.

"All models are wrong, some are useful"


## A simple model

Let's start with a simple model of DNA evolution. Imagine that when the DNA is being replicated, most of the time the appropriate nucleotide is incorporated. Some fraction of the time, at rate $\mu$, an event occurs where the appropriate nucleotides is replaced with a random nucleotide instead. The probability of selecting any of the nucleotides during one of these random replacement events is uniform (picking a C is just as probably as picking a G, for example), and the new nucleotide doesn't depend in any way on what nucleotide was there before. It is as if you had a bag containing equal frequencies of C, G, T, and A nucleotides. As you built the new DNA strand, every so often you would replace the nucleotide you should be adding with one you instead selected by reaching into the bag with your eyes closed and picking one at random.

Not all replacement events will result in an apparent change. Sometimes the appropriate nucleotide is selected by chance, even though it was picked at random. If, for example, the appropriate nucleotide was an A, under this model $1/4$ of the time a replacement event occurs an A is selected by chance and there is no apparent change. In such a case, there has not been a substitution (just a replacement in kind). If the A is replaced with any of the other three nucleotides we say there has been a substitution. Because three of the four possible outcomes of an event result in a substitution, the substitution rate is $3\beta$, which, because $\beta=\mu/4$, is equivalent to noting that the substitution is $(3/4) \mu$. Because some events result in no apparent change, substitutions are only a subset of events and the substitution rate is *lower* than the replacement event rate.

It might seem a bit odd to consider replacement events that don't result in substitutions, but this follows naturally from a central feature we specified for the the model - the new nucleotide doesn't depend in any way on what nucleotide was there before. If we had a process where replacements always resulted in substitutions, then excluding the a replacement in kind would require knowing which nucleotide should be placed so that we *don't* select it.

### Expected amount of change

One of the primary values of a model is that it allows us to think explicitly about how much evolutionary change we expect to see under the specified process. For the simple process described here, there are two things to consider if we want to know the amount of evolutionary change. The first is the substitution rate $\mu$ (which we also know if we know  $\beta$, since $\mu=4\beta$), and the time over which the evolutionary process acts. 

In Figure \@ref(fig:sim-jc-mu-sweep) the amount of evolutionary time is held constant, and the rate $\mu$ is changed. When $\mu=0$, the bottom bar, there are no replacements (black bars) and therefore no substitutions (the whole bar is the same color). 

```{r sim-jc-mu-sweep, fig.cap="Each horizontal bar is a simulation of evolution of a single nucleotide position through time, $t$, for a specified value of $\\mu$. Each sumulation starts out as an A. Black vertical bars correspond to replacement events, which don't all lead to substitutions (a new color). "}

  t = 100  # total time to consider
  mu_step = 0.01
  mu_values = seq( 0, 0.1, mu_step )
  
  segments = lapply( mu_values, function(x){
    sim_jc( mu=x, t, first = "A" ) %>% mutate ( mu=x  )
  }  ) %>%
    bind_rows()
  
  delta = ( mu_values[2] - mu_values[1] ) * 0.5
  
  ggplot( segments ) +
    scale_x_continuous(name="time") +
    geom_rect( mapping=aes(xmin=start, xmax=end, fill=nucleotide, ymin=mu, ymax=mu+delta ) ) +
    geom_segment( mapping=aes(x=start, xend=start, y=mu, yend=mu+delta ) ) +
    xlim( 0, t) +
    xlab( "time" )
    
  
```

As $\mu=0$ increases (going up on the $y$ axis), the number of replacement events over the same time interval increases (Figure \@ref(fig:sim-jc-mu-n)). This reflects the simple linear relationship $n=\mu t$, where $n$ is the number of expected replacement events.



```{r sim-jc-mu-n, fig.cap="The number of replacement events increases linearly with the replacement rate $\\mu$. This plot is from the same simulation as that shown in Figure \\@ref(fig:sim-jc-mu-sweep). The line is a linear model fit to the data."}

segments %>% 
  group_by(mu ) %>% 
  summarise( replacements = n()-1) %>% 
  ggplot( aes(x=mu, y=replacements) ) + 
    geom_point (  ) + 
    geom_smooth( method = "lm", se = FALSE )

```

Because of the linear relationship between the number of replacements and the product $\mu t$, rate and time are conflated. In many scenarios you can't estimate them independently. If there are a small number of replacements, for example, you can't be sure if there is a low rate over a long time interval or a high rate over a short interval. Both would give the same result. Because they are so often confounded in phylogenetic questions, often the rate is essentially fixed at one and the unit of time for edge lengths is given as the number of expected evolutionary change rather than absolute time (years, months, etc). You will often see this length as the scale bar of published phylogenies (Figure \@ref(fig:sim-tree-cnid)). The exception is when you have external information, such as dated fossils, that allow you to independently estimate edge lengths and rates.



```{r sim-tree-cnid, fig.cap="A published phylogeny [@zapata2015] with a scale bar indicating branch length in terms of the expected amount of evolutionary change, rather than absolute time."}

knitr::include_graphics("figures/Fig_cnidaria.png")

```

### Expected end state

The machinery above shows how a model can clarify the way we think about the expected amount of change. Many times, though, we want to know what the probability of a given end state is given a starting state, a model, and the amount of time elapsed. One way to anchor such a  question is to think about the extremes - what do we expect after a very small amount of change (either a short time or a slow rate of change, or both), and what do we expect after a large amount of change?

The situation is most clear after a small amount of change - we expect the end result to be the same as the starting condition. If we start with an A, for example, we expect to end with an A. In this situation, if we know the starting state that information tells us a lot about the end state. Not much else matters.

What should we expect, though, if there has been a large amount of change? Can we know anything at all? It turns out that we can. If there have been many replacements, one after the other, than the initial starting state really doesn't matter at all because whatever was there will probably have been replaced multiple times. If the starting state doesn't contain information about the end state, what does? It is the bag that you are picking the nucleotides at random from. Given enough evolutionary time, our simple model will lead the expected frequency of each nucleotide in the evolving sequence to be the same as the frequency in the bag that we randomly draw them from. Since we specified that you have the same chance of grabbing any nucleotide from the bag, eventually the probability of having each of the our nucleotides is the same, 25%. If you started with a sequence that had an A and let it evolve 100 times, after enough evolutionary time had passed to reach equilibrium you would expect to get 25 C's, 25 G's, 25 T's, and 25 A's.

```{r}
  mu = 0.050
  n_replicates = 1000
```


```{r sim-saturation, fig.cap=paste( "Stacked bar plots indicating the frequency of each nucleotide after evolution for a specified amount of time. The rate of evolution is $\\mu=" , mu, "$. There are ", n_replicates, " replicate simulations for each value of time. At time=0 (no evolution), the end result is always the same as the initial value, which is fixed at A in these simulations. As the length of time increases, the four nucleotides converge on equal frequencies of 25% each." ) }


  t_values = seq( 0, 100, 10 )
  t_values = rep( t_values, n_replicates )
  
  simulations = 
    lapply( t_values, function(x){
      sim_jc( mu=mu, t=x, first = "A", n=100 ) %>% mutate ( time=x  ) %>% slice_tail( n=1 )
    }  ) %>% 
    bind_rows()
  
  simulations %>% 
    ggplot() +
      aes(x = time, fill = factor(nucleotide)) +
      geom_bar(position = "fill")
  


```




## Generalizing the simple model





rates, equilibrium frequencies

```{r}

e = matrix(c(0.25,0.25,0.25,0.25),nrow=4)

R =matrix(c(
  -3,1,1,1,
  1,-3,1,1,
  1,1,-3,1,
  1,1,1,-3
),
nrow=4
)

Q = R %*% e

Q

```


exponentiation


## More complex models



## Model structure





## Aditional resources

- My own thinking about this material was heavilly influenced by Paul Lewis's wonderful lectures at the annual Workshop on Molecular Evolution at Woods Hole. Some of his lectures are now available online as part of the excellent [Phylo Seminar](https://www.youtube.com/channel/UCbAzhfySv7nLCrNYqZvBSMg), starting with https://www.youtube.com/watch?v=1r4z0YJq580&t=2111s

