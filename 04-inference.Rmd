# Inferring phylogenies from data

In the previous chapter we built modeling machinery that allowed us to specify a rate matrix $\mathbf{Q}$ (Equation \@ref(eq:sim-gtr)), and from that derive the matrix $\mathbf{P}(t)$  (Equation \@ref(eq:jc69-prob)) that gives the probability of a particular end state given $\mathbf{Q}$, the starting state, and the edge length $t$. We used $\mathbf{P}(t)$ in a generative context, to simulate the evolution of a DNA sequence along the edges of a tree.

We will now turn to using that exact same statistical framework for what may initially seem to be a very different task -- inferring the topology and and edge lengths of a phylogeny from DNA sequence data. But they are very similar conceptually. The basic intuition is that you can infer a phylogeny by looking for the topology and edge lengths that are most probable to generate the observed data given the model and model parameters.

This relationship between simulation and inference is widely used in a variety of fields. The probability of the observed data given a hypothesis is referred to as the Likelihood. Searching for the most likely hypothesis as referred to as Maximum Likelihood (ML).

## Probability of a single history

We will consider the toy phylogeny, along with its tip states, shown in Figure \@ref(fig:inference-toy).

```{r inference-toy, fig.cap="The toy phylogeny we will use to examine inference. Node numbers are in red. Edge lengths are in orange. Tip node states are within boxes."}

phy_text = "(((Species_A:0.5,Species_B:0.5):0.5,Species_C:1.0):0.2,Species_D:1.2);"
phy = read.tree( text=phy_text )

internal_states = expand.grid(
  node_5=c("A", "C", "G", "T"), 
  node_6=c("A", "C", "G", "T"), 
  node_7=c("A", "C", "G", "T")) %>% 
as.matrix() %>% 
t()

tip_states = c("T", "T", "A", "C")

node_states = rbind( replicate(ncol(internal_states), tip_states), internal_states )

get_edge_states = function( phy, states){
  D = tibble(
    parent = phy$edge[,1],
    child  = phy$edge[,2],
    start  = states[ phy$edge[,1] ],
    end    = states[ phy$edge[,2] ],
    edge_length = phy$edge.length
  )
  D
}

get_edge_prob  = function( start, end, edge_length, Q ){
  P = exponentiate_matrix(Q*edge_length)
  P[ start, end ]
}

get_edge_probs = function( phy, states, Q ){
  edges = get_edge_states( phy, states)
  prob = 
    edges %>%
    select( start, end, edge_length ) %>%
    pmap( get_edge_prob, Q )
  prob %<>% unlist()
  names(prob) = NULL
  edges$prob = prob
  edges
}

example_states = node_states[,60]

edge_probs = 
  get_edge_probs( phy, example_states, Q ) %>%
  arrange(child)


edge_lengths = rep( NA, max(edge_probs$child))
edge_lengths[ edge_probs$child ] = edge_probs$edge_length

p = rep( NA, max(edge_probs$child))
p[ edge_probs$child ] = edge_probs$prob

tip_state_labels = rep( NA, max(edge_probs$child))
tip_state_labels[1:length(tip_states)] = tip_states

ggtree(phy) +
  geom_tiplab(offset=0.2) +
  geom_label(aes(label=tip_state_labels)) +
  geom_text2(aes(label=node), col="red", nudge_x=0.12 ) +
  geom_text2(aes(label=edge_lengths), col="orange", nudge_x=-0.12, nudge_y=-0.1 ) +
  xlim(0,2)


```

We will start be calculating the probability of a single history of evolution for a single site on a single tree. By history a mean the full set of states at all nodes in a particular phylogeny for a particular nucleotide site. These are added to the toy phylogeny in Figure \@ref(fig:inference-internal-states). I want to emphasize that this isn't a history we have any particular reason to believe, it is just one possible history of states randomly chosen from all the possible histories.

```{r inference-internal-states, fig.cap="The same toy tree as above, but with arbitrary internal node states (in boxes)."}

ggtree(phy) +
  geom_tiplab(offset=0.2) +
  geom_label(aes(label=example_states)) +
  geom_text2(aes(label=node), col="red", nudge_x=0.15 ) +
  geom_text2(aes(label=edge_lengths), col="orange", nudge_x=-0.12, nudge_y=-0.1 ) +
  xlim(0,2)

```

Our goal now is to calculate the probability of each observed change. Recall that the matrix that contains these probabilities, given a starting state (rows), ending state (columns), and edge length $t$, is given by:

\begin{equation} 
  \mathbf{P}\left(t\right) = e^{\mathbf{Q} t} 
  (\#eq:prob)
\end{equation}

Where $\mathbf{Q}$ is the rate matrix. Let's plug some numbers in using the model we specified in the previous chapter. We don't have any specific reason to use this particular model on this tree, we are just sticking with it since we already built it.

Specifically:

```{r}
R
```

```{r}
Pi
```

And their product $\mathbf{Q}$, with the diagonal adjusted so that rows sum to 0:

```{r}
Q
```

For each in the edge, we can now use $\mathbf{P}(t)$ to calculate the probability of a change from the start state at the parent node to the end state at the child node, given the edge length $t$. The results are shown in Figure \@ref(fig:inference-history). 


```{r inference-history, fig.cap="The same toy tree as above, but with probabilities of the specific change along each edge (in blue)."}


ggtree(phy) +
  geom_tiplab(offset=0.2) +
  geom_label(aes(label=example_states)) +
  geom_text2(aes(label=node), col="red", nudge_x=0.15 ) +
  geom_text2(aes(label=edge_lengths), col="orange", nudge_x=-0.12, nudge_y=-0.1 ) +
  geom_text2(aes(label=round(p,3)), col="blue", nudge_x=-0.15, nudge_y=0.1 ) +
  xlim(0,2)

```

Now that we have the probabilities of each of these changes, we can calculate the joint probability of all these changes. When we want to calculate the joint probability of multiple independent events, we take the product of the probability of each specific event. For example, the probability of rolling a 4 on a fair die is $1/6$. The probability of rolling two 4s in a row is $1/6\times1/6=1/36$. So we can just multiply all the blue probabilities times each other. 

In Figure \@ref(fig:inference-history) we illustrated these probabilities as the probability of changes along each edge, but we can also think of these as the probabilities of the state at each node (including tips and internal nodes). 

```{r}
knitr::kable(data.frame( node=1:length(p), probability=p ))

pi_v = diag(Pi)
names(pi_v) = rownames(Pi)

root_node = which(is.na(p))
root_prob = pi_v[ example_states[root_node] ]

```

Note, though that the probability for node 5 is missing (it has a value of `NA`, which means it is not available). By reference to Figure \@ref(fig:inference-history) we can see that this is the root node. We calculated probabilities of each node state according to the probability of the state at the child node of each edge, and the root is not the child of any edge. So its probability is missing. We will therefore assess the probability of the root node state according to $\mathbf{\Pi}$, the equilibrium frequencies. This is the same approach we took when simulating data on a tree. When we fill that in our full set of probabilities is:

```{r}

p[which(is.na(p))] = root_prob

knitr::kable(data.frame( node=1:length(p), probability=p )) %>% 
  kable_styling()

example_prob = prod(p)

```

The joint probability of all these events happening (the root state being what it is, and then each of the changes along each of the edges) can now be calculated as their product. This comes out to $`r example_prob`$. Three are multiple ways to think about this probability. One is from a frequentest perspective. If we were to simulate character states on this tree, in the way we did in the previous chapter using this same model, we would expect this full set of character states `r round(example_prob*1e6,1)` times out of a million simulations.

This is a really small number, and this is a very small tree. As trees get larger there are many more probabilities we need to multiply, so the products get even smaller. The probabilities, in fact, get so small that computers have trouble storing them efficiently. Rather than store and manipulate the probabilities directly, then, most tools take the natural logs of the probabilities, $ln(p)$. The log probability of this particular history is `r log(example_prob)`. This transforms them to a numerical representation that is easier to work with. It also has the added value of making calculations simpler too. Given the relationship between the log of products of variables and the sum of logs of each value:

\begin{equation} 
  ln(a)+ln(b) = ln(ab)
  (\#eq:logs)
\end{equation}

We can take the log of each probability:

```{r}

p[which(is.na(p))] = root_prob

knitr::kable(data.frame( node=1:length(p), probability=p, `ln p`=log(p) )) %>% 
  kable_styling()

example_prob = prod(p)

```

And then sum them. This sum of these log probabilities is `r sum(log(p))`, the same is the log of products expected above. We will use this approach of summing log probabilities rather than taking the product of probabilities when we consider joint probabilities elsewhere.

Here we have used much of the same machinery as we did in the previous chapter, but toward a slightly different end. Rather than use the probability distributions provided by our model to generate nucleotides in a simulation, we instead calculated the log probability of a particular set of nucleotides. These may have seemed like very different tasks at first blush, but as you can now see their mathematical implementation shares many features.

## Probability of multiple histories

Above we considered the joint probability of a specific set of node states at all nodes, including both tip nodes and internal nodes. We specified all these node states arbitrarily and calculated the log probability of the entire configuration of states. Usually, though, we don't know the internal node states. We don't even know what internal nodes exist, which is why we are trying to infer the phylogeny! Instead we have observed states at the tips that we got by sequencing living organisms. We want to clamp these tip states and assess thei probability on a particular tree (with edge lengths) under the model. This probability is independent of a specific history of node states. If we aren't clamping the internal node states as well, how can we calculate the probability of just the tip node states? The key is to consider all possible internal states in turn. Each configuration of internal node states represents one possible history that gave rise to the observed tip states. 


## Scrap

A specific task - given a set of character data corresponding to the tips of a tree, what is the topology of the tree? Model is also estimated, but may or may not be of interest.

Calculating the likelihood of a tree


What a likelihood is

maximum likelihood

hueristics

Bayesian

"Model free" methods

